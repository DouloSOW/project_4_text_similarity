{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45dba4fd",
   "metadata": {},
   "source": [
    "# Metric 1 : BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c2953e",
   "metadata": {},
   "source": [
    "## I- Charging of the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eb3cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from fractions import Fraction\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf14d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06afde88",
   "metadata": {},
   "source": [
    "## I- Chargement des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cf640d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which','ensures', 'that', 'the', 'military', 'always','obeys', 'the', 'commands', 'of', 'the', 'party']\n",
    "ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that','ensures', 'that', 'the', 'military', 'will', 'forever','heed', 'Party', 'commands']\n",
    "ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which','guarantees', 'the', 'military', 'forces', 'always','being', 'under', 'the', 'command', 'of', 'the', 'Party']\n",
    "ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the', 'army', 'always', 'to', 'heed', 'the', 'directions','of', 'the', 'party']\n",
    "\n",
    "hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was','interested', 'in', 'world', 'history']\n",
    "ref2a = ['he', 'was', 'interested', 'in', 'world', 'history','because', 'he', 'read', 'the', 'book']\n",
    "\n",
    "list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\n",
    "hypotheses = [hyp1, hyp2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a620abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights=(0.25, 0.25, 0.25, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f661d2b",
   "metadata": {},
   "source": [
    "## II- Implementation of the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11407fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To see if there is no error in the input data\n",
    "\n",
    "if len(list_of_references) != len(hypotheses) :\n",
    "    print(\"Error, you need the same number of ref and hyp\")\n",
    "if len(list_of_references) == 0 :\n",
    "    print(\"Error, you need references\")\n",
    "if len(hypotheses) == 0 :\n",
    "    print(\"Error, you need hypotheses\")\n",
    "\n",
    "#weights = [weights]\n",
    "weight_length = len(weights)\n",
    "\n",
    "# We create Counter dictionnaries (count the occurencies) \n",
    "numerators_precision = Counter()\n",
    "denominators_precision = Counter()\n",
    "hyp_lengths, ref_lengths = 0, 0\n",
    "\n",
    "# Iterate through each hypothesis and their corresponding references.\n",
    "for references, hypothesis in zip(list_of_references, hypotheses):\n",
    "    # compute the numerator and denominator of the corpus-level precision for each order of ngram. \n",
    "    for i in range(1, weight_length + 1):\n",
    "        # Extracts all ngrams in hypothesis\n",
    "        counts = Counter(ngrams(hypothesis, i))\n",
    "        # To get the union of counts of hyp and ref ngrams\n",
    "        max_counts = {}\n",
    "        for reference in references:\n",
    "            # Extract all unique ngrams in references\n",
    "            reference_counts = (Counter(ngrams(reference, i)))\n",
    "            for ngram in counts:\n",
    "                max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n",
    "\n",
    "        # Intersection between hypothesis and references' counts for each different ngrams.\n",
    "        intersection_counts = {ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()}\n",
    "\n",
    "        numerator = sum(intersection_counts.values())\n",
    "        # The denominator must be superior to 0 (0 posssible if the ngram order is > len(reference) ).\n",
    "        denominator = max(1, sum(counts.values()))\n",
    "        p_i = Fraction(numerator, denominator, _normalize=False)\n",
    "        numerators_precision[i] += p_i.numerator\n",
    "        denominators_precision[i] += p_i.denominator\n",
    "\n",
    "    # Compute the hypothesis length (number of words) and the closest reference length.\n",
    "    # It is useful to calculate corpus-level brevity penalty\n",
    "    hyp_len = len(hypothesis)\n",
    "    hyp_lengths += hyp_len\n",
    "    #print(hyp_lengths)\n",
    "    ref_lens = (len(reference) for reference in references)\n",
    "    ref_lengths += min(ref_lens, key= lambda ref_len: (abs(ref_len - hyp_len), ref_len))\n",
    "    #print(ref_lengths)\n",
    "\n",
    "# Calculate corpus-level brevity penalty.\n",
    "if hyp_lengths > ref_lengths:\n",
    "    brevity_penalty = 1\n",
    "else:\n",
    "    brevity_penalty = math.exp(1 - ref_lengths / hyp_lengths)\n",
    "\n",
    "# Save the precision values for the different ngram orders (from 1 to weight_length).\n",
    "p_n = [Fraction(numerators_precision[i], denominators_precision[i], _normalize=False) for i in range(1, weight_length + 1)]\n",
    "\n",
    "# In the case that precision is equal to 0\n",
    "if numerators_precision[1] == 0:\n",
    "        bleu_score = 0\n",
    "\n",
    "for weight in weights:\n",
    "    info = (weight * math.log(precision_i) for precision_i in p_n if precision_i > 0)\n",
    "bleu_score = brevity_penalty * math.exp(math.fsum(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ec64b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5920778868801042"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032960d",
   "metadata": {},
   "source": [
    "Explanations : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e763990e",
   "metadata": {},
   "source": [
    "modified_precision : \n",
    "    \n",
    "    The normal precision method may lead to some wrong translations with\n",
    "    high-precision, e.g., the translation, in which a word of reference\n",
    "    repeats several times, has very high precision.\n",
    "\n",
    "    This function only returns the Fraction object that contains the numerator\n",
    "    and denominator necessary to calculate the corpus-level precision.\n",
    "    To calculate the modified precision for a single pair of hypothesis and\n",
    "    references, cast the Fraction object into a float.\n",
    "\n",
    "    The famous \"the the the ... \" example shows that you can get BLEU precision\n",
    "    by duplicating high frequency words.\n",
    "    \n",
    "    In the modified n-gram precision, a reference word will be considered\n",
    "    exhausted after a matching hypothesis word is identified\n",
    "\n",
    "closest_ref_length : \n",
    "    \n",
    "    This function finds the reference that is the closest length to the\n",
    "    hypothesis. The closest reference length is referred to as *r* variable\n",
    "    from the brevity penalty formula in Papineni et. al. (2002)\n",
    "\n",
    "brevity_penalty : \n",
    "\n",
    "    As the modified n-gram precision still has the problem from the short\n",
    "    length sentence, brevity penalty is used to modify the overall BLEU\n",
    "    score according to length.\n",
    "\n",
    "    An example from the paper. There are three references with length 12, 15\n",
    "    and 17. And a concise hypothesis of the length 12. The brevity penalty is 1.\n",
    "    \n",
    "    In case a hypothesis translation is shorter than the references, penalty is\n",
    "    applied.\n",
    "    \n",
    "    The length of the closest reference is used to compute the penalty. If the\n",
    "    length of a hypothesis is 12, and the reference lengths are 13 and 2, the\n",
    "    penalty is applied because the hypothesis length (12) is less then the\n",
    "    closest reference length (13).\n",
    "    \n",
    "    The brevity penalty doesn't depend on reference order. More importantly,\n",
    "    when two reference sentences are at the same distance, the shortest\n",
    "    reference sentence length is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d03460",
   "metadata": {},
   "source": [
    "Inspired from https://www.nltk.org/_modules/nltk/translate/bleu_score.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
